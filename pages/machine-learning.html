<html>
  <head>
      <link href="https://fonts.googleapis.com/css?family=Cutive+Mono|Poiret+One|Jacques+Francois|VT323" rel="stylesheet">
      <script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML' async></script>
    <style>
      ::-webkit-scrollbar {
        display: none;
      }
      body {
        font-family: 'Jacques Francois', monospace;
        font-size: 14pt;
        padding: 20px 20%;
        word-wrap: break-word;
        text-align: justify;
        user-select: none;
        padding-bottom: 100px;
      }
      a {
        font-family: 'Poiret One', sans-serif;
        display: block;
        color: gray;
        text-align: center;
      }
      pre {
        margin-top: 20px;
        margin-bottom: 0;
        font-family: Courier, monospace;
        font-weight: 800;
      }
      h1, h2 {
        margin-top: 80px;
      }
      h1, h2, h3, h4 {
        font-weight: 900;
        font-family: 'Poiret One', sans-serif;
        text-transform: uppercase;
      }
      h4  {
        font-style: italic
      }
      h1 {
        text-align: center;
      }
      .quote {
        margin: 20px 30px;
        font-family: 'Homemade Apple', cursive;
      }
      .author {
        font-style: italic;
        text-align: center;
      }
      .embed{
        margin-top: -20px;
        margin-bottom: -30px;
        margin-left: 100px;
      }
      .citation {
        text-indent: -30px;
        margin-left: 60px;
        margin-right: 40px;
      }
    </style>
  </head>
  <body>
    <h1>Machine Learning Notes (Incomplete)</h1>
    <a href="https://www.youtube.com/playlist?list=PLLssT5z_DsK-h9vYZkQkYNWcItqhlRJLN">[YouTube] Machine Learning by Andrew Ng, Stanford University</a>
    <h2>0. Optimiazation Algorithms</h2>
    Suppose \(h_\theta(x)\) is the hypothesis parametrized by \(\theta\), and \(J(\theta)\) is the corresponding cost function . Supervised learning approximates the target function mapping inputs to outputs. The problem can be formulated as the following optimization problem:
    \[ \text{arg}\min_\theta J(\theta) \]
    <h3>Batch Gradient Descent</h3>
    Make corrections w.r.t. the entire dataset.
    <pre>
    Repeat {
    </pre>
    <div class="embed">\(\theta_j := \theta_j - \alpha \dfrac{\partial}{\partial \theta_j} J(\theta)\)</div>
    <pre>
    }
    </pre>
    <h3>Stochastic Gradient Descent</h3>
    Make corrections w.r.t. each data point.
    <pre>
    Randomly shuffle dataset
    Repeat {
      for i = 1:m {
    </pre>
    <div class="embed">\(\theta_j := \theta_j - \alpha \dfrac{\partial}{\partial \theta_j} cost(\theta, x^{(i)}, y^{(i)})\)</div>
    <pre>
      }
    }
    </pre>
    <h3>Mini-batch Gradient Descent</h3>
    Make corrections w.r.t. batches of \(b\) data points.
    <pre>
    Randomly shuffle dataset
    Repeat {
      for i = 1:b:m {
    </pre>
    <div class="embed">\(\theta_j := \theta_j - \alpha \dfrac{1}{b}\displaystyle\sum_{k=i}^{i+b-1}\dfrac{\partial}{\partial \theta_j} cost(\theta, x^{(k)}, y^{(k)})\)</div>
    <pre>
      }
    }
    </pre>
    <h2>1. Linear Regression</h2>
    <h3>Model</h3>
    \[ h_\theta(x) = \theta^T x \]
    <h3>Cost Function</h3>
    \[ J(\theta) = \frac{1}{2m} \sum_{i=1}^m \Big(h_\theta(x^{(i)}) - y^{(i)} \Big)^2\]
    
    <h2>2. Logistic Regression</h2>
    <h3>Model</h3>
    \[ h_\theta(x) = g(\theta^T x) \quad\text{where}\quad g(z) = \frac{1}{1+e^{-z}}\]
    Predict
    \[ y = \begin{cases}
      1, \quad h_\theta(x) \geq 0.5\\
      0, \quad h_\theta(x) < 0.5
    \end{cases}\]
    <h3>Cost Function</h3>
    \[ J(\theta) = \frac{1}{m}\sum_{i=1}^m cost\left(h_\theta(x^{(i)}), y^{(i)}\right) \]
    where
    \[ cost\left(h_\theta\left(x\right), y\right) = \begin{cases}
    -\log(h_\theta(x)) \quad\text{if } y = 1\\
    -\log(1-h_\theta(x)) \quad\text{if } y = 0\end{cases}\]
    With an additional regularization term,
    \[ J(\theta) = - \frac{1}{m}\sum_{i=1}^m \Big[y^{(i)}\log\left(h_\theta(x^{(i)})\right) + \left(1-y^{(i)}\right)\log(1-h_\theta\left(x^{(i)})\right)\Big]\\ + \frac{\lambda}{2m} \sum_{j=1}^n \theta_j^2\]
    
    <h2>3. Support Vector Machine</h2>
    <h3>Cost Function</h3>
    \[ J(\theta) = - C\sum_{i=1}^m \Big[y^{(i)}cost_1(\theta^Tx^{(i)}) + \left(1-y^{(i)}\right)cost_0(\theta^Tx^{(i)})\Big] + \frac{1}{2} \sum_{j=1}^n \theta_j^2\]
    where \(cost_0\) and \(cost_1\) are linear functions whose negative values are converted to zero.
    <h3>Optimization Objective</h3>
    Since \(cost_i\) can be made zero, the optimization obejctive can be formulated as
    \[ \min_\theta \frac{1}{2} \sum_{j=1}^n \theta_j^2 = \min_\theta \frac{1}{2} \lVert\theta\rVert^2 \]
    while satisfying
    \[ \theta^T x^{(i)} = \lVert\theta\rVert\cdot p^{(i)} \begin{cases} \geq +1, \quad\text{if } y^{(i)} = 1\\ \leq -1, \quad\text{if } y^{(i)} = 0\end{cases}\]
    Minimizing \(\lVert\theta\rVert\) is thus equivalent to maximizing \(p^{(i)}\), which results in a large margin.
    
    <h2>4. Neural Networks</h2>
    <h3>Model</h3>
    \[ z^{(2)} = \Theta^{(1)}x,\quad z^{(j)} = \Theta^{(j-1)}a^{(j-1)}, \quad a^{(j)} = g(z^{(j)}), \quad\text{where \(g\) is non-linear} \]
    \[h_\Theta(x) = \Big(g \circ \Theta^{(m)} \circ \cdots \circ g \circ \Theta^{(1)}\Big)x\]
    <h3>Cost Function</h3>
    Suppose the network has \(L\) layers and \(h_{\Theta}(x) \in \mathbb{R}^K \)
    \[ J(\Theta) = - \frac{1}{m}\sum_{i=1}^m \sum_{k=1}^K \Big[y_k^{(i)}\log\left(h_\theta(x^{(i)})_k\right) + \left(1-y_k^{(i)}\right)\log\left(1-h_\theta(x^{(i)})_k\right)\Big] \\+ \frac{\lambda}{2m} \sum_{l=1}^{L-1} \sum_{i=1}^{s_{l}} \sum_{j=1}^{s_{l+1}} \left(\Theta^{(l)}_{ji}\right)^2\] 
    <h3>Training Algorithm</h3>
    <pre>
    Randomly initialize weights
    for i = 1:m {
      Forward propagation
      Compute cost function
      Backpropagation
    }
    </pre>
    Backpropagation computes partial derivatives of the cost function
    \(\dfrac{\partial}{\partial\Theta^{(l)}_{ij}}J(\Theta)\)
    <h1><i>Below are unsupervised learning algorithms</i></h1>
    <h2>5. Clustering: K-Means</h2>
    <h3>Optimization Objective</h3>
    \[ J\left(c^{(1)},...c^{(m)},\mu_1,...\mu_K\right) := \frac{1}{m}\sum_{i=1}^m \left\lVert x^{(i)}-\mu_{c^{(i)}}\right\rVert^2\]
    The algorithm computes \(\min_{c^{(1)},...c^{(m)}}J\) and \(\min_{\mu_1,...\mu_K}J\) in two steps.
    <h3>Training Algorithm</h3>
    <pre>
    Randomly initialize K cluster centroids
    Repeat {
      for i = 1:m {
        c[i] := index of cluster centroid closest to x[i]
      }
      for k = 1:K {
        mu[k] := average of points assigned to cluster k
      }
    }
    </pre>
    <h2>6. Dimensionality Reduction: PCA</h2>
    Compute the covariance matrix
    \[ \Sigma := \frac{1}{m} \sum_{i=1}^m\big(x^{(i)}\big)\big(x^{(i)}\big)^T\]
    Then project data points to the eigenvectors corresponding to the largest eigenvalues.

    <h2>7. Anomaly Detection</h2>
    <h3>Training</h3>
    Fit distribution parameters \(\mu_i\) and \(\sigma_i\) for each feature \(x_i\):
    \[ \mu_j = \frac{1}{m}\sum_{i=1}^m x^{(i)}_j, \quad \sigma_j^2 = \frac{1}{m}\sum_{i=1}^m \big(x^{(i)}_j - \mu_j \big)^2 \]
    <h3>Prediction</h3>
    Given a new example \(x\), compute probability density
    \[ p(x) = \prod_{i=1}^n p(x_j;\mu_j, \sigma_j^2) = \prod_{i=1}^n \frac{1}{\sqrt{2\pi\sigma_j}}\exp\left[{-\frac{(x_j-\mu_j)^2}{2\sigma_j^2}}\right]\]
    Predict anomaly if \(p(x) < \epsilon \).
    
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        CommonHTML: {
          scale: 110
        }
      });
    </script>
  </body>
  </html>
